{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a56d821",
   "metadata": {},
   "source": [
    "# Workshop: Open Source LLM using Llama 3.2, Groq, and Ollama\n",
    "This workshop covers using Llama 3.2 model with Groq acceleration and Ollama platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a925a9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Llama 3.2: Meta's latest multimodal LLM.\n",
    "- Groq: Hardware AI accelerator.\n",
    "- Ollama: Local LLM platform with CLI and API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372c2c5",
   "metadata": {},
   "source": [
    "## Step 1: Install Ollama\n",
    "Download from https://ollama.com and install.\n",
    "Verify with `ollama --version` in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command outside the notebook\n",
    "# ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fae1a8",
   "metadata": {},
   "source": [
    "## Step 2: Run Llama 3.2\n",
    "- `ollama run llama3.2`\n",
    "- Variants: `llama3.2:1b`, `llama3.2-vision`\n",
    "- List models: `ollama list`\n",
    "- Describe image: `ollama run llama3.2-vision \"describe this image: /path/to/image.png\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9df11",
   "metadata": {},
   "source": [
    "## Step 3: Use Groq Accelerator\n",
    "- Sign up on https://groq.com\n",
    "- Install Groq Python client: `pip install groq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"your_api_key\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.2-70b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing simply.\"}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52083cf3",
   "metadata": {},
   "source": [
    "## Step 4: Image Captioning with Groq\n",
    "Send image bytes with prompt to describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8993785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"image.jpg\", \"rb\") as img_file:\n",
    "    image_data = img_file.read()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.2-90b-vision\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe this image.\"}],\n",
    "    image=image_data,\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba85893",
   "metadata": {},
   "source": [
    "## Step 5: Simple Chatbot with Ollama & LangChain\n",
    "Install packages: `pip install langchain langchain-ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Question: {question}\\nAnswer: Let's think step by step.\")\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "chain = prompt | model\n",
    "\n",
    "while True:\n",
    "    question = input(\"Enter your question (or 'exit' to quit): \")\n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    print(\"Thinking...\")\n",
    "    answer = chain.invoke({\"question\": question})\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef4ab0",
   "metadata": {},
   "source": [
    "## Step 6: Deploy with Streamlit\n",
    "Install Streamlit: `pip install streamlit`\n",
    "Create app.py with code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b53188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "st.title(\"Llama 3.2 Chatbot\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Question: {question}\\nAnswer: Let's think step by step.\")\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "chain = prompt | model\n",
    "\n",
    "question = st.text_input(\"Ask a question:\")\n",
    "if question:\n",
    "    answer = chain.invoke({\"question\": question})\n",
    "    st.write(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c104da",
   "metadata": {},
   "source": [
    "Run with `streamlit run app.py`"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
